{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will using word2vec for vectoring the text instead of TF-IDF, to see if there is improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full corpus size:  2225\n",
      "citizenship event for 18s touted citizenship ceremonies could be introduced for people celebrating their 18th birthday  charles clarke has said.  the idea will be tried as part of an overhaul of the way government approaches  inclusive citizenship  particularly for ethnic minorities. a pilot scheme based on ceremonies in australia will start in october. mr clarke said it would be a way of recognising young people reaching their voting age when they also gain greater independence from parents. britain s young black and asian people are to be encouraged to learn about the nation s heritage as part of the government s new race strategy which will also target specific issues within different ethnic minority groups. officials say the home secretary wants young people to feel they belong and to understand their  other cultural identities  alongside being british. the launch follows a row about the role of faith schools in britain. on monday school inspection chief david bell  accused some islamic schools of failing to teach pupils about their obligations to british society.  the muslim council of britain said ofsted boss mr bell s comments were  highly irresponsible . the home office started work on its community cohesion and race equality strategy last year and the outcome  launched on wednesday  is called  improving opportunity  strengthening society . it is aimed at tackling racism  exclusion  segregation and the rise in political and religious extremism.  it represents a move away from the one-size-fits-all approach to focus on specifics within cultural groups   said a home office spokesman.  it is not right to say that if you are from a black or ethnic minority group you must be disadvantaged.  the spokesman highlighted specific issues that affect particular communities - for example people of south asian origin tend to suffer from a high incidence of heart disease.   it is about drilling down and focusing on these sorts of problems   the spokesman added. launching the initiative mr clarke said enormous progress had been made on race issues in recent years. he added:  but while many members of black and minority ethnic communities are thriving  some may still find it harder to succeed in employment or gain access to healthcare  education or housing.  this strategy sets out the government s commitment to doing more to identify and respond to the specific needs of minorities in our society.  some 8% of the uk population described themselves as coming from a non-white ethnic minority in the 2001 census.  the downing street strategy unit in 2003 said people from indian and chinese backgrounds were doing well on average  often outperforming white people in education and earnings. but those of pakistani  bangladeshi and black caribbean origin were significantly more likely to be unemployed and earn less than whites  it said. the home office wants more initiatives which try to promote a sense of belonging by encouraging young people to take part in voluntary work. the programmes are designed to support the citizenship lessons already taking place in schools.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#Load data\n",
    "train_set = pd.read_csv('corpus/clean_training_set.csv')\n",
    "test_set = pd.read_csv('corpus/clean_test_set.csv')\n",
    "\n",
    "#Concat all news in both data set into a big list\n",
    "full_news = list(train_set.Text) + list(test_set.Text)\n",
    "\n",
    "print(\"full corpus size: \", len(full_news))\n",
    "\n",
    "print(full_news[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "Different form TF-IDF, training Word2Vec is needed sentences in sequence form. So we do not remove stop words from text data. But make some transform like:\n",
    "- Remove url\n",
    "- Remove punct\n",
    "- Ner tag words number to (time, date, quantity, ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "print(f\"Spacy version: {spacy.__version__}\")\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    #text = correct_spelling(text)\n",
    "    \n",
    "    doc = nlp_spacy(text)\n",
    "    \n",
    "    clean_bag_words = []\n",
    "    for token in doc:\n",
    "        if token.is_currency:\n",
    "            clean_bag_words.append('_currency_')\n",
    "        elif token.ent_type_ == 'ORDINAL':\n",
    "            clean_bag_words.append('_ordinal_')\n",
    "        elif token.ent_type_ == 'TIME':\n",
    "            clean_bag_words.append('_time_')\n",
    "        elif token.ent_type_ == 'QUANTITY':\n",
    "            clean_bag_words.append('_quantity_')\n",
    "        elif token.ent_type_ == 'DATE':\n",
    "            clean_bag_words.append('_date_')\n",
    "        elif token.is_alpha:\n",
    "            clean_bag_words.append(token.lemma_)\n",
    "    \n",
    "    return clean_bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['citizenship', 'event', 'for', '_date_', 'tout', 'citizenship', 'ceremony', 'could', 'be', 'introduce', 'for', 'people', 'celebrate', '-PRON-', '_ordinal_', 'birthday', 'charles', 'clarke', 'have', 'say', 'the', 'idea', 'will', 'be', 'try', 'as', 'part', 'of', 'an', 'overhaul', 'of', 'the', 'way', 'government', 'approach', 'inclusive', 'citizenship', 'particularly', 'for', 'ethnic', 'minority', 'a', 'pilot', 'scheme', 'base', 'on', 'ceremony', 'in', 'australia', 'will', 'start', 'in', 'october', 'mr', 'clarke', 'say', '-PRON-', 'would', 'be', 'a', 'way', 'of', 'recognise', 'young', 'people', 'reach', '-PRON-', 'voting', 'age', 'when', '-PRON-', 'also', 'gain', 'great', 'independence', 'from', 'parent', 'britain', 's', 'young', 'black', 'and', 'asian', 'people', 'be', 'to', 'be', 'encourage', 'to', 'learn', 'about', 'the', 'nation', 's', 'heritage', 'as', 'part', 'of', 'the', 'government', 's', 'new', 'race', 'strategy', 'which', 'will', 'also', 'target', 'specific', 'issue', 'within', 'different', 'ethnic', 'minority', 'group', 'official', 'say', 'the', 'home', 'secretary', 'want', 'young', 'people', 'to', 'feel', '-PRON-', 'belong', 'and', 'to', 'understand', '-PRON-', 'other', 'cultural', 'identity', 'alongside', 'be', 'british', 'the', 'launch', 'follow', 'a', 'row', 'about', 'the', 'role', 'of', 'faith', 'school', 'in', 'britain', 'on', '_date_', 'school', 'inspection', 'chief', 'david', 'bell', 'accuse', 'some', 'islamic', 'school', 'of', 'fail', 'to', 'teach', 'pupil', 'about', '-PRON-', 'obligation', 'to', 'british', 'society', 'the', 'muslim', 'council', 'of', 'britain', 'say', 'ofste', 'boss', 'mr', 'bell', 's', 'comment', 'be', 'highly', 'irresponsible', 'the', 'home', 'office', 'start', 'work', 'on', '-PRON-', 'community', 'cohesion', 'and', 'race', 'equality', 'strategy', '_date_', '_date_', 'and', 'the', 'outcome', 'launch', 'on', '_date_', 'be', 'call', 'improve', 'opportunity', 'strengthen', 'society', '-PRON-', 'be', 'aim', 'at', 'tackle', 'racism', 'exclusion', 'segregation', 'and', 'the', 'rise', 'in', 'political', 'and', 'religious', 'extremism', '-PRON-', 'represent', 'a', 'move', 'away', 'from', 'the', 'one', 'size', 'fit', 'all', 'approach', 'to', 'focus', 'on', 'specific', 'within', 'cultural', 'group', 'say', 'a', 'home', 'office', 'spokesman', '-PRON-', 'be', 'not', 'right', 'to', 'say', 'that', 'if', '-PRON-', 'be', 'from', 'a', 'black', 'or', 'ethnic', 'minority', 'group', '-PRON-', 'must', 'be', 'disadvantaged', 'the', 'spokesman', 'highlight', 'specific', 'issue', 'that', 'affect', 'particular', 'community', 'for', 'example', 'people', 'of', 'south', 'asian', 'origin', 'tend', 'to', 'suffer', 'from', 'a', 'high', 'incidence', 'of', 'heart', 'disease', '-PRON-', 'be', 'about', 'drill', 'down', 'and', 'focus', 'on', 'these', 'sort', 'of', 'problem', 'the', 'spokesman', 'add', 'launch', 'the', 'initiative', 'mr', 'clarke', 'say', 'enormous', 'progress', 'have', 'be', 'make', 'on', 'race', 'issue', 'in', '_date_', '_date_', '-PRON-', 'add', 'but', 'while', 'many', 'member', 'of', 'black', 'and', 'minority', 'ethnic', 'community', 'be', 'thrive', 'some', 'may', 'still', 'find', '-PRON-', 'hard', 'to', 'succeed', 'in', 'employment', 'or', 'gain', 'access', 'to', 'healthcare', 'education', 'or', 'housing', 'this', 'strategy', 'set', 'out', 'the', 'government', 's', 'commitment', 'to', 'do', 'more', 'to', 'identify', 'and', 'respond', 'to', 'the', 'specific', 'need', 'of', 'minority', 'in', '-PRON-', 'society', 'some', 'of', 'the', 'uk', 'population', 'describe', '-PRON-', 'as', 'come', 'from', 'a', 'non', 'white', 'ethnic', 'minority', 'in', 'the', '_date_', 'census', 'the', 'down', 'street', 'strategy', 'unit', 'in', '_date_', 'say', 'people', 'from', 'indian', 'and', 'chinese', 'background', 'be', 'do', 'well', 'on', 'average', 'often', 'outperform', 'white', 'people', 'in', 'education', 'and', 'earning', 'but', 'those', 'of', 'pakistani', 'bangladeshi', 'and', 'black', 'caribbean', 'origin', 'be', 'significantly', 'more', 'likely', 'to', 'be', 'unemployed', 'and', 'earn', 'less', 'than', 'white', '-PRON-', 'say', 'the', 'home', 'office', 'want', 'more', 'initiative', 'which', 'try', 'to', 'promote', 'a', 'sense', 'of', 'belong', 'by', 'encourage', 'young', 'people', 'to', 'take', 'part', 'in', 'voluntary', 'work', 'the', 'programme', 'be', 'design', 'to', 'support', 'the', 'citizenship', 'lesson', 'already', 'take', 'place', 'in', 'school']\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessing(full_news[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:18<00:00, 28.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import thread_map\n",
    "full_news_clean = thread_map(text_preprocessing, full_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=200, alpha=0.025)', 'datetime': '2022-04-28T21:12:13.045406', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "MAX_WORKERS = multiprocessing.cpu_count() - 2\n",
    "\n",
    "#Init model\n",
    "word2vec_model = gensim.models.Word2Vec(window=3, vector_size=200, negative=10, min_count=1, \n",
    "                                        sample=1e-4, workers = MAX_WORKERS, sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 21963 word types from a corpus of 865336 raw words and 2225 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 21963 unique words (100.0%% of original 21963, drops 0)', 'datetime': '2022-04-28T21:12:13.352754', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 865336 word corpus (100.0%% of original 865336, drops 0)', 'datetime': '2022-04-28T21:12:13.353721', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 21963 items\n",
      "INFO:gensim.models.word2vec:sample=0.0001 downsamples 508 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 422990.81793832785 word corpus (48.9%% of prior 865336)', 'datetime': '2022-04-28T21:12:13.495522', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 21963 words and 200 dimensions: 46122300 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-04-28T21:12:13.745842', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 615 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#Build vocab\n",
    "word2vec_model.build_vocab(full_news_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 6 workers on 21963 vocabulary and 200 features, using sg=1 hs=0 sample=0.0001 negative=10 window=3 shrink_windows=True', 'datetime': '2022-04-28T21:12:13.841222', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 92.22% examples, 388568 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 1 : training on 865336 raw words (422552 effective words) took 1.1s, 391652 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 83.60% examples, 344577 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 2 : training on 865336 raw words (423534 effective words) took 1.2s, 353639 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 88.76% examples, 368010 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 3 : training on 865336 raw words (422828 effective words) took 1.1s, 378688 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 89.84% examples, 376773 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 4 : training on 865336 raw words (423348 effective words) took 1.1s, 385671 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 88.76% examples, 373401 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 5 : training on 865336 raw words (423053 effective words) took 1.2s, 367846 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 6 - PROGRESS: at 74.83% examples, 312028 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 6 : training on 865336 raw words (422907 effective words) took 1.3s, 333474 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 7 - PROGRESS: at 88.76% examples, 368607 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 7 : training on 865336 raw words (423177 effective words) took 1.1s, 373921 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 8 - PROGRESS: at 88.76% examples, 373122 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 8 : training on 865336 raw words (422979 effective words) took 1.1s, 370230 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 9 - PROGRESS: at 83.60% examples, 350581 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 9 : training on 865336 raw words (422706 effective words) took 1.2s, 362033 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 10 - PROGRESS: at 68.58% examples, 277113 words/s, in_qsize 12, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 10 : training on 865336 raw words (423248 effective words) took 1.4s, 298718 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 11 - PROGRESS: at 74.83% examples, 313454 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 11 : training on 865336 raw words (423083 effective words) took 1.3s, 319178 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 12 - PROGRESS: at 77.12% examples, 322200 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 12 : training on 865336 raw words (423338 effective words) took 1.4s, 313556 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 13 - PROGRESS: at 77.12% examples, 320217 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 13 : training on 865336 raw words (423653 effective words) took 1.3s, 333100 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 14 - PROGRESS: at 82.43% examples, 341428 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 14 : training on 865336 raw words (422562 effective words) took 1.2s, 346431 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 15 - PROGRESS: at 84.81% examples, 351910 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 15 : training on 865336 raw words (422523 effective words) took 1.2s, 361094 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 16 - PROGRESS: at 88.00% examples, 369301 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 16 : training on 865336 raw words (422929 effective words) took 1.1s, 368727 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 17 - PROGRESS: at 83.60% examples, 351665 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 17 : training on 865336 raw words (423271 effective words) took 1.2s, 358995 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 18 - PROGRESS: at 84.40% examples, 344238 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 18 : training on 865336 raw words (423514 effective words) took 1.2s, 352790 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 19 - PROGRESS: at 88.76% examples, 361779 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 19 : training on 865336 raw words (422805 effective words) took 1.2s, 359538 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 20 - PROGRESS: at 84.40% examples, 349072 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 20 : training on 865336 raw words (423027 effective words) took 1.2s, 353701 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 21 - PROGRESS: at 86.74% examples, 363872 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 21 : training on 865336 raw words (422601 effective words) took 1.2s, 363063 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 22 - PROGRESS: at 84.72% examples, 354084 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 22 : training on 865336 raw words (422876 effective words) took 1.2s, 358146 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 23 - PROGRESS: at 84.40% examples, 349849 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 23 : training on 865336 raw words (423018 effective words) took 1.2s, 358132 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 24 - PROGRESS: at 86.74% examples, 359914 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 24 : training on 865336 raw words (421869 effective words) took 1.2s, 362959 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 25 - PROGRESS: at 88.00% examples, 365948 words/s, in_qsize 11, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 25 : training on 865336 raw words (423846 effective words) took 1.2s, 367696 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 21633400 raw words (10575247 effective words) took 30.0s, 352968 effective words/s', 'datetime': '2022-04-28T21:12:43.804031', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10575247, 21633400)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_model.train(full_news_clean, total_examples=len(full_news_clean), epochs=25, report_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:KeyedVectors lifecycle event {'fname_or_handle': 'model/word2vec_bbc_200', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-28T21:12:43.918592', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:saved model/word2vec_bbc_200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab, and dimension features: (21963, 200)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total vocab, and dimension features: {word2vec_model.wv.vectors.shape}\")\n",
    "word2vec_model.wv.save('model/word2vec_bbc_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strasbourg', 0.5911527276039124),\n",
       " ('contravention', 0.567435622215271),\n",
       " ('genome', 0.5610306859016418),\n",
       " ('shami', 0.5599691271781921),\n",
       " ('connectedness', 0.5560138821601868),\n",
       " ('companion', 0.553825855255127),\n",
       " ('damming', 0.5493731498718262),\n",
       " ('frailty', 0.5487865805625916),\n",
       " ('convention', 0.5443421602249146),\n",
       " ('chakrabarti', 0.5389552712440491)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('treadmill', 0.7206295728683472),\n",
       " ('timeslot', 0.7108034491539001),\n",
       " ('napalm', 0.6681059002876282),\n",
       " ('definately', 0.6680393218994141),\n",
       " ('truncated', 0.657975971698761),\n",
       " ('edgy', 0.6570670008659363),\n",
       " ('footfall', 0.6546801924705505),\n",
       " ('glide', 0.6490354537963867),\n",
       " ('afternoon', 0.6483133435249329),\n",
       " ('onimusha', 0.6481848359107971)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fleshiness', 0.5509140491485596),\n",
       " ('luminosity', 0.5503056049346924),\n",
       " ('speakerboxxx', 0.5440655946731567),\n",
       " ('ample', 0.5337980389595032),\n",
       " ('stabber', 0.5236081480979919),\n",
       " ('tug', 0.5185402631759644),\n",
       " ('courtney', 0.5174943208694458),\n",
       " ('forbidden', 0.5116926431655884),\n",
       " ('intuition', 0.511223554611206),\n",
       " ('downsize', 0.5042084455490112)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('namely', 0.5362796187400818),\n",
       " ('irancell', 0.5216964483261108),\n",
       " ('company', 0.5190095901489258),\n",
       " ('tankan', 0.5056881904602051),\n",
       " ('tetsuro', 0.5030200481414795),\n",
       " ('nanjing', 0.5013078451156616),\n",
       " ('broking', 0.49983251094818115),\n",
       " ('radar', 0.4983746111392975),\n",
       " ('outsourcing', 0.49788033962249756),\n",
       " ('pessimism', 0.4975112974643707)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading KeyedVectors object from model/word2vec_bbc_200\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'fname': 'model/word2vec_bbc_200', 'datetime': '2022-04-28T21:12:44.491596', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('broom', 0.4992559552192688),\n",
       " ('arbitrarily', 0.4974686801433563),\n",
       " ('hyped', 0.47862720489501953),\n",
       " ('prosperous', 0.4762115180492401),\n",
       " ('agnostic', 0.47368767857551575),\n",
       " ('gainful', 0.4725506007671356),\n",
       " ('mactaggart', 0.47231656312942505),\n",
       " ('speeding', 0.469606876373291),\n",
       " ('moldova', 0.46904489398002625),\n",
       " ('certificate', 0.46093687415122986)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.KeyedVectors.load('model/word2vec_bbc_200')\n",
    "word2vec_model.most_similar('home')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>BagWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>['worldcom', 'boss', 'launch', 'defence', 'law...</td>\n",
       "      <td>['worldcom', 'boss', 'launch', 'defence', 'law...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>['german', 'business', 'confidence', 'slide', ...</td>\n",
       "      <td>['german', 'business', 'confidence', 'slide', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>['bbc', 'poll', 'indicate', 'economic', 'gloom...</td>\n",
       "      <td>['bbc', 'poll', 'indicate', 'economic', 'gloom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>['lifestyle', 'govern', 'mobile', 'choice', 'f...</td>\n",
       "      <td>['lifestyle', 'govern', 'mobile', 'choice', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>['enron', 'boss', '_currency_', 'payout', 'eig...</td>\n",
       "      <td>['enron', 'boss', '_currency_', 'payout', 'eig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category  \\\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business   \n",
       "1        154  german business confidence slides german busin...  business   \n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business   \n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech   \n",
       "4        917  enron bosses in $168m payout eighteen former e...  business   \n",
       "\n",
       "                                           CleanText  \\\n",
       "0  ['worldcom', 'boss', 'launch', 'defence', 'law...   \n",
       "1  ['german', 'business', 'confidence', 'slide', ...   \n",
       "2  ['bbc', 'poll', 'indicate', 'economic', 'gloom...   \n",
       "3  ['lifestyle', 'govern', 'mobile', 'choice', 'f...   \n",
       "4  ['enron', 'boss', '_currency_', 'payout', 'eig...   \n",
       "\n",
       "                                            BagWords  \n",
       "0  ['worldcom', 'boss', 'launch', 'defence', 'law...  \n",
       "1  ['german', 'business', 'confidence', 'slide', ...  \n",
       "2  ['bbc', 'poll', 'indicate', 'economic', 'gloom...  \n",
       "3  ['lifestyle', 'govern', 'mobile', 'choice', 'f...  \n",
       "4  ['enron', 'boss', '_currency_', 'payout', 'eig...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1490/1490 [00:49<00:00, 30.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [worldcom, ex, boss, launch, defence, lawyer, ...\n",
       "1    [german, business, confidence, slide, german, ...\n",
       "2    [bbc, poll, indicate, economic, gloom, citizen...\n",
       "3    [lifestyle, govern, mobile, choice, faster, be...\n",
       "4    [enron, boss, in, _currency_, m, payout, eight...\n",
       "Name: BagWords, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "train_set['BagWords'] = thread_map(text_preprocessing, train_set['Text']) \n",
    "train_set['BagWords'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoring sentence with Word2Vec\n",
    "\n",
    "There are an algorithm known as AvgWord2Vec that vectorizes sentences by taking average all vector word in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "def word2vec_sentence(bag_words):\n",
    "    vector = np.zeros(word2vec_model.vectors.shape[1])\n",
    "    \n",
    "    for token in bag_words:\n",
    "        try:\n",
    "            vector = np.add(vector,word2vec_model[token])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.divide(vector, len(bag_words))\n",
    "\n",
    "print(word2vec_sentence(train_set['BagWords'][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import thread_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1490/1490 [00:05<00:00, 289.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1490, 200)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectoring traning set\n",
    "train_vectors = np.array(thread_map(word2vec_sentence, train_set['BagWords'].values))\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1490,)\n"
     ]
    }
   ],
   "source": [
    "#Init label encoder for news category feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#Encode target, save to varible y\n",
    "y = encoder.fit_transform(train_set['Category'].values)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import  GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Evalution Model\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "#CV splitrun model 10x with 70/20 split intentionally leaving out 10%\n",
    "cv_split = ShuffleSplit(n_splits = 5, test_size = .2,\n",
    "                        train_size = .7, random_state = 0)\n",
    "\n",
    "param_grids = [\n",
    "    #Random Forest\n",
    "    {'n_estimators': [100, 150, 250],\n",
    "     'criterion': ['gini', 'entropy']},\n",
    "\n",
    "    #GausianNB\n",
    "    {},\n",
    "\n",
    "    #SVC\n",
    "    {'C': [1, 10, 100],\n",
    "     'gamma': [0.01, 0.1, 0.001]},\n",
    "\n",
    "]\n",
    "\n",
    "MLA = [\n",
    "    RandomForestClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "MAX_WORKER = multiprocessing.cpu_count() - 2\n",
    "\n",
    "report = pd.read_csv('report_training.csv')\n",
    "scoring = {'f1': 'f1_macro', 'precision': 'precision_macro', 'recall':'recall_macro'}\n",
    "\n",
    "row = len(report)\n",
    "for mla, param in zip(MLA, param_grids):\n",
    "    gscv = GridSearchCV(mla, param, cv = cv_split, return_train_score=True, n_jobs=MAX_WORKER, \n",
    "                        scoring=scoring, refit='f1', error_score='raise')\n",
    "    gscv.fit(train_vectors,y)\n",
    "    \n",
    "    best_index = gscv.best_index_\n",
    "    \n",
    "    report.loc[row, 'algorithm'] = gscv.best_estimator_.__class__.__name__ + \"_Word2Vec\"\n",
    "    report.loc[row, 'best_params'] = str(gscv.best_params_)\n",
    "    report.loc[row, 'f1_train'] = gscv.cv_results_['mean_train_f1'][best_index]\n",
    "    report.loc[row, 'f1_test'] = gscv.cv_results_['mean_test_f1'][best_index]\n",
    "    report.loc[row, 'recall_train'] = gscv.cv_results_['mean_train_recall'][best_index]\n",
    "    report.loc[row, 'recall_test'] = gscv.cv_results_['mean_test_recall'][best_index]\n",
    "    report.loc[row, 'precision_train'] = gscv.cv_results_['mean_train_precision'][best_index]\n",
    "    report.loc[row, 'precision_test'] = gscv.cv_results_['mean_test_precision'][best_index]\n",
    "    report.loc[row, 'fit_time'] = gscv.cv_results_['mean_fit_time'][best_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>best_params</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 250}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>1.661590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.931458</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>0.931203</td>\n",
       "      <td>0.859601</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.861169</td>\n",
       "      <td>0.013115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>0.986628</td>\n",
       "      <td>0.900158</td>\n",
       "      <td>0.986545</td>\n",
       "      <td>0.900327</td>\n",
       "      <td>0.986740</td>\n",
       "      <td>0.902152</td>\n",
       "      <td>0.262039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 150}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950623</td>\n",
       "      <td>2.689887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaussianNB_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903421</td>\n",
       "      <td>0.193932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>6.497143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier_Word2Vec</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 150}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967868</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970771</td>\n",
       "      <td>3.264943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GaussianNB_Word2Vec</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.958047</td>\n",
       "      <td>0.947563</td>\n",
       "      <td>0.958102</td>\n",
       "      <td>0.947490</td>\n",
       "      <td>0.958212</td>\n",
       "      <td>0.948213</td>\n",
       "      <td>0.012621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVC_Word2Vec</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>0.985137</td>\n",
       "      <td>0.971320</td>\n",
       "      <td>0.985105</td>\n",
       "      <td>0.971288</td>\n",
       "      <td>0.985193</td>\n",
       "      <td>0.971927</td>\n",
       "      <td>0.098401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    algorithm  \\\n",
       "0      RandomForestClassifier_TF_IDF_ONE_GRAM   \n",
       "1                  GaussianNB_TF_IDF_ONE_GRAM   \n",
       "2                         SVC_TF_IDF_ONE_GRAM   \n",
       "3  RandomForestClassifier_TF_IDF_ONE_TWO_GRAM   \n",
       "4              GaussianNB_TF_IDF_ONE_TWO_GRAM   \n",
       "5                     SVC_TF_IDF_ONE_TWO_GRAM   \n",
       "6             RandomForestClassifier_Word2Vec   \n",
       "7                         GaussianNB_Word2Vec   \n",
       "8                                SVC_Word2Vec   \n",
       "\n",
       "                                     best_params  f1_train   f1_test  \\\n",
       "0     {'criterion': 'gini', 'n_estimators': 250}  1.000000  0.911843   \n",
       "1                                             {}  0.931458  0.858537   \n",
       "2                        {'C': 10, 'gamma': 0.1}  0.986628  0.900158   \n",
       "3  {'criterion': 'entropy', 'n_estimators': 150}  1.000000  0.947797   \n",
       "4                                             {}  1.000000  0.901938   \n",
       "5                        {'C': 10, 'gamma': 0.1}  1.000000  0.970375   \n",
       "6  {'criterion': 'entropy', 'n_estimators': 150}  1.000000  0.969028   \n",
       "7                                             {}  0.958047  0.947563   \n",
       "8                        {'C': 10, 'gamma': 0.1}  0.985137  0.971320   \n",
       "\n",
       "   recall_train  recall_test  precision_train  precision_test  fit_time  \n",
       "0      1.000000     0.910633         1.000000        0.914503  1.661590  \n",
       "1      0.931203     0.859601         0.933053        0.861169  0.013115  \n",
       "2      0.986545     0.900327         0.986740        0.902152  0.262039  \n",
       "3      1.000000     0.946281         1.000000        0.950623  2.689887  \n",
       "4      1.000000     0.905434         1.000000        0.903421  0.193932  \n",
       "5      1.000000     0.970559         1.000000        0.970600  6.497143  \n",
       "6      1.000000     0.967868         1.000000        0.970771  3.264943  \n",
       "7      0.958102     0.947490         0.958212        0.948213  0.012621  \n",
       "8      0.985105     0.971288         0.985193        0.971927  0.098401  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increassing score overall, on average 1%\n",
    "- Get rid of overfitting\n",
    "- Time handle reduce\n",
    "- Memory cost reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv('report_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec did make a better performance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "299cc0f12f7d41819069429ffe373621fa25f2d2daa3da5f840bd97d4c8cb810"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
