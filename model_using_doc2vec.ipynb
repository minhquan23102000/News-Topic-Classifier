{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will using doc2vec for vectoring the text to see if there is improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full corpus size:  2225\n",
      "citizenship event for 18s touted citizenship ceremonies could be introduced for people celebrating their 18th birthday  charles clarke has said.  the idea will be tried as part of an overhaul of the way government approaches  inclusive citizenship  particularly for ethnic minorities. a pilot scheme based on ceremonies in australia will start in october. mr clarke said it would be a way of recognising young people reaching their voting age when they also gain greater independence from parents. britain s young black and asian people are to be encouraged to learn about the nation s heritage as part of the government s new race strategy which will also target specific issues within different ethnic minority groups. officials say the home secretary wants young people to feel they belong and to understand their  other cultural identities  alongside being british. the launch follows a row about the role of faith schools in britain. on monday school inspection chief david bell  accused some islamic schools of failing to teach pupils about their obligations to british society.  the muslim council of britain said ofsted boss mr bell s comments were  highly irresponsible . the home office started work on its community cohesion and race equality strategy last year and the outcome  launched on wednesday  is called  improving opportunity  strengthening society . it is aimed at tackling racism  exclusion  segregation and the rise in political and religious extremism.  it represents a move away from the one-size-fits-all approach to focus on specifics within cultural groups   said a home office spokesman.  it is not right to say that if you are from a black or ethnic minority group you must be disadvantaged.  the spokesman highlighted specific issues that affect particular communities - for example people of south asian origin tend to suffer from a high incidence of heart disease.   it is about drilling down and focusing on these sorts of problems   the spokesman added. launching the initiative mr clarke said enormous progress had been made on race issues in recent years. he added:  but while many members of black and minority ethnic communities are thriving  some may still find it harder to succeed in employment or gain access to healthcare  education or housing.  this strategy sets out the government s commitment to doing more to identify and respond to the specific needs of minorities in our society.  some 8% of the uk population described themselves as coming from a non-white ethnic minority in the 2001 census.  the downing street strategy unit in 2003 said people from indian and chinese backgrounds were doing well on average  often outperforming white people in education and earnings. but those of pakistani  bangladeshi and black caribbean origin were significantly more likely to be unemployed and earn less than whites  it said. the home office wants more initiatives which try to promote a sense of belonging by encouraging young people to take part in voluntary work. the programmes are designed to support the citizenship lessons already taking place in schools.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#Load data\n",
    "train_set = pd.read_csv('corpus/clean_training_set.csv')\n",
    "test_set = pd.read_csv('corpus/clean_test_set.csv')\n",
    "\n",
    "#Concat all news in both data set into a big list\n",
    "full_news = list(train_set.Text) + list(test_set.Text)\n",
    "\n",
    "print(\"full corpus size: \", len(full_news))\n",
    "\n",
    "print(full_news[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "Different form TF-IDF, training Word2Vec is needed sentences in sequence form. So we do not remove stop words from text data. But make some transform like:\n",
    "- Remove url\n",
    "- Remove punct\n",
    "- Ner tag words number to (time, date, quantity, ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "print(f\"Spacy version: {spacy.__version__}\")\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    #text = correct_spelling(text)\n",
    "    \n",
    "    doc = nlp_spacy(text)\n",
    "    \n",
    "    clean_bag_words = []\n",
    "    for token in doc:\n",
    "        if token.is_currency:\n",
    "            clean_bag_words.append('_currency_')\n",
    "        elif token.ent_type_ == 'ORDINAL':\n",
    "            clean_bag_words.append('_ordinal_')\n",
    "        elif token.ent_type_ == 'TIME':\n",
    "            clean_bag_words.append('_time_')\n",
    "        elif token.ent_type_ == 'QUANTITY':\n",
    "            clean_bag_words.append('_quantity_')\n",
    "        elif token.ent_type_ == 'DATE':\n",
    "            clean_bag_words.append('_date_')\n",
    "        elif token.is_alpha:\n",
    "            clean_bag_words.append(token.lemma_)\n",
    "    \n",
    "    return clean_bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:03<00:00, 34.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib.concurrent import thread_map\n",
    "full_news_clean = thread_map(text_preprocessing, full_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "MAX_WORKERS = multiprocessing.cpu_count() - 2\n",
    "\n",
    "tag_documents = []\n",
    "#Tagged document for doc2vec_model\n",
    "for i, doc in enumerate(full_news_clean):\n",
    "    tag_documents.append(gensim.models.doc2vec.TaggedDocument(doc, [i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['worldcom', 'ex', 'boss', 'launch', 'defence', 'lawyer', 'defend', 'former', 'worldcom', 'chief', 'bernie', 'ebber', 'against', 'a', 'battery', 'of', 'fraud', 'charge', 'have', 'call', 'a', 'company', 'whistleblower', 'as', '-PRON-', '_ordinal_', 'witness', 'cynthia', 'cooper', 'worldcom', 's', 'ex', 'head', 'of', 'internal', 'accounting', 'alert', 'director', 'to', 'irregular', 'accounting', 'practice', 'at', 'the', '-PRON-', 'telecom', 'giant', 'in', '_date_', '-PRON-', 'warning', 'lead', 'to', 'the', 'collapse', 'of', 'the', 'firm', 'follow', 'the', 'discovery', 'of', 'an', '_currency_', '_currency_', '_date_', 'accounting', 'fraud', 'mr', 'ebber', 'have', 'plead', 'not', 'guilty', 'to', 'charge', 'of', 'fraud', 'and', 'conspiracy', 'prosecution', 'lawyer', 'have', 'argue', 'that', 'mr', 'ebber', 'orchestrate', 'a', 'series', 'of', 'accounting', 'trick', 'at', 'worldcom', 'order', 'employee', 'to', 'hide', 'expense', 'and', 'inflate', 'revenue', 'to', 'meet', 'wall', 'street', 'earning', 'estimate', 'but', 'ms', 'cooper', 'who', 'now', 'run', '-PRON-', 'own', 'consulting', 'business', 'tell', 'a', 'jury', 'in', 'new', 'york', 'on', '_date_', 'that', 'external', 'auditor', 'arthur', 'andersen', 'have', 'approve', 'worldcom', 's', 'account', 'in', '_date_', '_date_', '_date_', '_date_', '-PRON-', 'say', 'andersen', 'have', 'give', 'a', 'green', 'light', 'to', 'the', 'procedure', 'and', 'practice', 'use', 'by', 'worldcom', 'mr', 'ebber', 's', 'lawyer', 'have', 'say', '-PRON-', 'be', 'unaware', 'of', 'the', 'fraud', 'argue', 'that', 'auditor', 'do', 'not', 'alert', '-PRON-', 'to', 'any', 'problem', 'ms', 'cooper', 'also', 'say', 'that', 'during', 'shareholder', 'meeting', 'mr', 'ebber', 'often', 'pass', 'over', 'technical', 'question', 'to', 'the', 'company', 's', 'finance', 'chief', 'give', 'only', 'brief', 'answer', '-PRON-', 'the', 'prosecution', 's', 'star', 'witness', 'former', 'worldcom', 'financial', 'chief', 'scott', 'sullivan', 'have', 'say', 'that', 'mr', 'ebber', 'order', 'accounting', 'adjustment', 'at', 'the', 'firm', 'tell', '-PRON-', 'to', 'hit', '-PRON-', 'book', 'however', 'ms', 'cooper', 'say', 'mr', 'sullivan', 'have', 'not', 'mention', 'anything', 'uncomfortable', 'about', 'worldcom', 's', 'account', 'during', 'a', '_date_', 'audit', 'committee', 'meeting', 'mr', 'ebber', 'could', 'face', 'a', 'jail', 'sentence', 'of', '_date_', '_date_', 'if', 'convict', 'of', 'all', 'the', 'charge', '-PRON-', 'be', 'face', 'worldcom', 'emerge', 'from', 'bankruptcy', 'protection', 'in', '_date_', 'and', 'be', 'now', 'know', 'as', 'mci', '_date_', '_date_', 'mci', 'agree', 'to', 'a', 'buyout', 'by', 'verizon', 'communication', 'in', 'a', 'deal', 'value', 'at', '_currency_'], tags=[0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Init model\n",
    "doc2vec_model = gensim.models.Doc2Vec(window=3, vector_size=200, negative=10, min_count=1, \n",
    "                                        sample=1e-4, workers = MAX_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 21963 word types and 2225 unique tags from a corpus of 2225 examples and 865336 words\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 21963 unique words (100.0%% of original 21963, drops 0)', 'datetime': '2022-04-29T10:57:49.526160', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 865336 word corpus (100.0%% of original 865336, drops 0)', 'datetime': '2022-04-29T10:57:49.528125', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 21963 items\n",
      "INFO:gensim.models.word2vec:sample=0.0001 downsamples 508 most-common words\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 422990.81793832785 word corpus (48.9%% of prior 865336)', 'datetime': '2022-04-29T10:57:49.670372', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 21963 words and 200 dimensions: 48347300 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 641 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#Build vocab\n",
    "doc2vec_model.build_vocab(tag_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'training model with 6 workers on 21963 vocabulary and 200 features, using sg=0 hs=0 sample=0.0001 negative=10 window=3 shrink_windows=True', 'datetime': '2022-04-29T10:59:03.053339', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 1 : training on 865336 raw words (425533 effective words) took 0.6s, 704189 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 2 : training on 865336 raw words (424769 effective words) took 0.5s, 817712 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 3 : training on 865336 raw words (425139 effective words) took 0.5s, 803919 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 4 : training on 865336 raw words (424963 effective words) took 0.6s, 747226 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 5 : training on 865336 raw words (425542 effective words) took 0.6s, 773694 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 6 : training on 865336 raw words (425061 effective words) took 0.6s, 681886 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 7 : training on 865336 raw words (425197 effective words) took 0.6s, 723765 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 8 : training on 865336 raw words (424884 effective words) took 0.6s, 736488 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 9 : training on 865336 raw words (425342 effective words) took 0.5s, 784296 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 10 : training on 865336 raw words (425181 effective words) took 0.6s, 688541 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 11 : training on 865336 raw words (425564 effective words) took 0.6s, 716611 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 12 : training on 865336 raw words (425130 effective words) took 0.6s, 761431 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 13 : training on 865336 raw words (424820 effective words) took 0.6s, 744698 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 14 : training on 865336 raw words (425211 effective words) took 0.7s, 627613 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 15 : training on 865336 raw words (425168 effective words) took 0.5s, 777346 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 16 : training on 865336 raw words (425741 effective words) took 0.5s, 862388 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 17 : training on 865336 raw words (424912 effective words) took 0.5s, 773974 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 18 - PROGRESS: at 88.00% examples, 371222 words/s, in_qsize 12, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 18 : training on 865336 raw words (424986 effective words) took 1.1s, 388305 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 19 : training on 865336 raw words (425434 effective words) took 0.6s, 717599 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 20 : training on 865336 raw words (425219 effective words) took 0.5s, 896081 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 21 : training on 865336 raw words (425343 effective words) took 0.5s, 932925 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 22 : training on 865336 raw words (424650 effective words) took 0.5s, 915585 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 23 : training on 865336 raw words (424702 effective words) took 0.5s, 892510 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 24 : training on 865336 raw words (424905 effective words) took 0.5s, 904957 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 25 : training on 865336 raw words (425836 effective words) took 0.5s, 902467 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 26 : training on 865336 raw words (424783 effective words) took 0.5s, 903999 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 27 : training on 865336 raw words (425244 effective words) took 0.7s, 618201 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 28 : training on 865336 raw words (425030 effective words) took 0.7s, 640584 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 29 : training on 865336 raw words (424801 effective words) took 0.4s, 958961 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 30 : training on 865336 raw words (425314 effective words) took 0.5s, 904809 effective words/s\n",
      "INFO:gensim.utils:Doc2Vec lifecycle event {'msg': 'training on 25960080 raw words (12754404 effective words) took 17.1s, 746694 effective words/s', 'datetime': '2022-04-29T10:59:20.135416', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2vec_model.train(tag_documents, total_examples=len(tag_documents), epochs=30, report_delay=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2076, 1: 149})\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "for doc_id in range(len(tag_documents)):\n",
    "    inferred_vector = doc2vec_model.infer_vector(tag_documents[doc_id].words)\n",
    "    \n",
    "    #Get top 5 most similar documents\n",
    "    sims = doc2vec_model.dv.most_similar([inferred_vector], topn=5)\n",
    "    \n",
    "    #Get rank of this documents by itself\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    \n",
    "import collections\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 90% of the inferred documents are found to be most similar to itself and about 10% of the time it is mistakenly most similar to another document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Doc2Vec lifecycle event {'fname_or_handle': 'model/doc2vec_bbc_200', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-29T11:11:15.571144', 'gensim': '4.1.2', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22598-SP0', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved model/doc2vec_bbc_200\n"
     ]
    }
   ],
   "source": [
    "#Save doc2vec model\n",
    "doc2vec_model.save('model/doc2vec_bbc_200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>['worldcom', 'boss', 'launch', 'defence', 'law...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>['german', 'business', 'confidence', 'slide', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>['bbc', 'poll', 'indicate', 'economic', 'gloom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>['lifestyle', 'govern', 'mobile', 'choice', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>['enron', 'boss', '_currency_', 'payout', 'eig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category  \\\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business   \n",
       "1        154  german business confidence slides german busin...  business   \n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business   \n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech   \n",
       "4        917  enron bosses in $168m payout eighteen former e...  business   \n",
       "\n",
       "                                           CleanText  \n",
       "0  ['worldcom', 'boss', 'launch', 'defence', 'law...  \n",
       "1  ['german', 'business', 'confidence', 'slide', ...  \n",
       "2  ['bbc', 'poll', 'indicate', 'economic', 'gloom...  \n",
       "3  ['lifestyle', 'govern', 'mobile', 'choice', 'f...  \n",
       "4  ['enron', 'boss', '_currency_', 'payout', 'eig...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1490/1490 [00:51<00:00, 29.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [worldcom, ex, boss, launch, defence, lawyer, ...\n",
       "1    [german, business, confidence, slide, german, ...\n",
       "2    [bbc, poll, indicate, economic, gloom, citizen...\n",
       "3    [lifestyle, govern, mobile, choice, faster, be...\n",
       "4    [enron, boss, in, _currency_, m, payout, eight...\n",
       "Name: BagWords, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "train_set['BagWords'] = thread_map(text_preprocessing, train_set['Text']) \n",
    "train_set['BagWords'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectoring sentence with Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "def doc2vec_sentence(bag_words):\n",
    "    return doc2vec_model.infer_vector(bag_words)\n",
    "\n",
    "print(doc2vec_sentence(train_set['BagWords'][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1490/1490 [00:10<00:00, 138.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1490, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectoring traning set\n",
    "train_vectors = np.array(thread_map(doc2vec_sentence, train_set['BagWords'].values))\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1490,)\n"
     ]
    }
   ],
   "source": [
    "#Init label encoder for news category feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#Encode target, save to varible y\n",
    "y = encoder.fit_transform(train_set['Category'].values)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import  GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Evalution Model\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "#CV splitrun model 10x with 70/20 split intentionally leaving out 10%\n",
    "cv_split = ShuffleSplit(n_splits = 5, test_size = .2,\n",
    "                        train_size = .7, random_state = 0)\n",
    "\n",
    "param_grids = [\n",
    "    #Random Forest\n",
    "    {'n_estimators': [100, 150, 250],\n",
    "     'criterion': ['gini', 'entropy']},\n",
    "\n",
    "    #GausianNB\n",
    "    {},\n",
    "\n",
    "    #SVC\n",
    "    {'C': [1, 10, 100],\n",
    "     'gamma': [0.01, 0.1, 0.001]},\n",
    "\n",
    "]\n",
    "\n",
    "MLA = [\n",
    "    RandomForestClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "MAX_WORKER = multiprocessing.cpu_count() - 2\n",
    "\n",
    "report = pd.read_csv('report_training.csv')\n",
    "scoring = {'f1': 'f1_macro', 'precision': 'precision_macro', 'recall':'recall_macro'}\n",
    "\n",
    "row = len(report)\n",
    "for mla, param in zip(MLA, param_grids):\n",
    "    gscv = GridSearchCV(mla, param, cv = cv_split, return_train_score=True, n_jobs=MAX_WORKER, \n",
    "                        scoring=scoring, refit='f1', error_score='raise')\n",
    "    gscv.fit(train_vectors,y)\n",
    "    \n",
    "    best_index = gscv.best_index_\n",
    "    \n",
    "    report.loc[row, 'algorithm'] = gscv.best_estimator_.__class__.__name__ + \"_Doc2Vec\"\n",
    "    report.loc[row, 'best_params'] = str(gscv.best_params_)\n",
    "    report.loc[row, 'f1_train'] = gscv.cv_results_['mean_train_f1'][best_index]\n",
    "    report.loc[row, 'f1_test'] = gscv.cv_results_['mean_test_f1'][best_index]\n",
    "    report.loc[row, 'recall_train'] = gscv.cv_results_['mean_train_recall'][best_index]\n",
    "    report.loc[row, 'recall_test'] = gscv.cv_results_['mean_test_recall'][best_index]\n",
    "    report.loc[row, 'precision_train'] = gscv.cv_results_['mean_train_precision'][best_index]\n",
    "    report.loc[row, 'precision_test'] = gscv.cv_results_['mean_test_precision'][best_index]\n",
    "    report.loc[row, 'fit_time'] = gscv.cv_results_['mean_fit_time'][best_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>best_params</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 250}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>1.661590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.931458</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>0.931203</td>\n",
       "      <td>0.859601</td>\n",
       "      <td>0.933053</td>\n",
       "      <td>0.861169</td>\n",
       "      <td>0.013115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC_TF_IDF_ONE_GRAM</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>0.986628</td>\n",
       "      <td>0.900158</td>\n",
       "      <td>0.986545</td>\n",
       "      <td>0.900327</td>\n",
       "      <td>0.986740</td>\n",
       "      <td>0.902152</td>\n",
       "      <td>0.262039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 150}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950623</td>\n",
       "      <td>2.689887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaussianNB_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903421</td>\n",
       "      <td>0.193932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC_TF_IDF_ONE_TWO_GRAM</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>6.497143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier_Word2Vec</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 150}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967868</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970771</td>\n",
       "      <td>3.264943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GaussianNB_Word2Vec</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.958047</td>\n",
       "      <td>0.947563</td>\n",
       "      <td>0.958102</td>\n",
       "      <td>0.947490</td>\n",
       "      <td>0.958212</td>\n",
       "      <td>0.948213</td>\n",
       "      <td>0.012621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVC_Word2Vec</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>0.985137</td>\n",
       "      <td>0.971320</td>\n",
       "      <td>0.985105</td>\n",
       "      <td>0.971288</td>\n",
       "      <td>0.985193</td>\n",
       "      <td>0.971927</td>\n",
       "      <td>0.098401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier_Word2Vec</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 250}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965187</td>\n",
       "      <td>2.682367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GaussianNB_Word2Vec</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.950814</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.950847</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.950929</td>\n",
       "      <td>0.921289</td>\n",
       "      <td>0.004598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVC_Word2Vec</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01}</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.967870</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.968360</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.968445</td>\n",
       "      <td>0.098202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier_Doc2Vec</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 250}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965376</td>\n",
       "      <td>2.289609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GaussianNB_Doc2Vec</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.950814</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.950847</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.950929</td>\n",
       "      <td>0.921289</td>\n",
       "      <td>0.004603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC_Doc2Vec</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01}</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.967870</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.968360</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.968445</td>\n",
       "      <td>0.100799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     algorithm  \\\n",
       "0       RandomForestClassifier_TF_IDF_ONE_GRAM   \n",
       "1                   GaussianNB_TF_IDF_ONE_GRAM   \n",
       "2                          SVC_TF_IDF_ONE_GRAM   \n",
       "3   RandomForestClassifier_TF_IDF_ONE_TWO_GRAM   \n",
       "4               GaussianNB_TF_IDF_ONE_TWO_GRAM   \n",
       "5                      SVC_TF_IDF_ONE_TWO_GRAM   \n",
       "6              RandomForestClassifier_Word2Vec   \n",
       "7                          GaussianNB_Word2Vec   \n",
       "8                                 SVC_Word2Vec   \n",
       "9              RandomForestClassifier_Word2Vec   \n",
       "10                         GaussianNB_Word2Vec   \n",
       "11                                SVC_Word2Vec   \n",
       "12              RandomForestClassifier_Doc2Vec   \n",
       "13                          GaussianNB_Doc2Vec   \n",
       "14                                 SVC_Doc2Vec   \n",
       "\n",
       "                                      best_params  f1_train   f1_test  \\\n",
       "0      {'criterion': 'gini', 'n_estimators': 250}  1.000000  0.911843   \n",
       "1                                              {}  0.931458  0.858537   \n",
       "2                         {'C': 10, 'gamma': 0.1}  0.986628  0.900158   \n",
       "3   {'criterion': 'entropy', 'n_estimators': 150}  1.000000  0.947797   \n",
       "4                                              {}  1.000000  0.901938   \n",
       "5                         {'C': 10, 'gamma': 0.1}  1.000000  0.970375   \n",
       "6   {'criterion': 'entropy', 'n_estimators': 150}  1.000000  0.969028   \n",
       "7                                              {}  0.958047  0.947563   \n",
       "8                         {'C': 10, 'gamma': 0.1}  0.985137  0.971320   \n",
       "9      {'criterion': 'gini', 'n_estimators': 250}  1.000000  0.963234   \n",
       "10                                             {}  0.950814  0.920289   \n",
       "11                        {'C': 1, 'gamma': 0.01}  0.999123  0.967870   \n",
       "12     {'criterion': 'gini', 'n_estimators': 250}  1.000000  0.963957   \n",
       "13                                             {}  0.950814  0.920289   \n",
       "14                        {'C': 1, 'gamma': 0.01}  0.999123  0.967870   \n",
       "\n",
       "    recall_train  recall_test  precision_train  precision_test  fit_time  \n",
       "0       1.000000     0.910633         1.000000        0.914503  1.661590  \n",
       "1       0.931203     0.859601         0.933053        0.861169  0.013115  \n",
       "2       0.986545     0.900327         0.986740        0.902152  0.262039  \n",
       "3       1.000000     0.946281         1.000000        0.950623  2.689887  \n",
       "4       1.000000     0.905434         1.000000        0.903421  0.193932  \n",
       "5       1.000000     0.970559         1.000000        0.970600  6.497143  \n",
       "6       1.000000     0.967868         1.000000        0.970771  3.264943  \n",
       "7       0.958102     0.947490         0.958212        0.948213  0.012621  \n",
       "8       0.985105     0.971288         0.985193        0.971927  0.098401  \n",
       "9       1.000000     0.962467         1.000000        0.965187  2.682367  \n",
       "10      0.950847     0.921212         0.950929        0.921289  0.004598  \n",
       "11      0.999063     0.968360         0.999189        0.968445  0.098202  \n",
       "12      1.000000     0.963699         1.000000        0.965376  2.289609  \n",
       "13      0.950847     0.921212         0.950929        0.921289  0.004603  \n",
       "14      0.999063     0.968360         0.999189        0.968445  0.100799  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Score and time on overall same as AvgWord2Vec \n",
    "- Memory cost go up because Doc2Vec model consume more memory\n",
    "- Doc2Vec is scalable than Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv('report_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is same as Word2Vec, but on a large scale of data, there will be sure different performance between there two models. \n",
    "But to keep simple as possible and scale ability, I prefer Doc2Vec."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "299cc0f12f7d41819069429ffe373621fa25f2d2daa3da5f840bd97d4c8cb810"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
